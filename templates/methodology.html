<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Methodology | Smart Code Assistant</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background-color: #f4f6f9;
            color: #333;
            margin: 0;
            padding: 20px;
        }
        .container {
            max-width: 960px;
            margin: auto;
            background: #fff;
            padding: 30px;
            border-radius: 12px;
            box-shadow: 0 8px 24px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2c3e50;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
        }
        h2 {
            color: #34495e;
        }
        p {
            line-height: 1.6;
        }
        code {
            background: #eee;
            padding: 2px 6px;
            border-radius: 4px;
            font-size: 90%;
        }
        nav ul {
        list-style-type: none;
        padding: 0;
        }

        nav ul li {
            display: inline;
            margin: 0 15px;
        }
    </style>
</head>
<body>
    <div class="container">
        <nav>
            <ul>
                <li><a href="/">Home</a></li>
                <li><a href="/about">About</a></li>
                <li><a href="/contact">Contact</a></li>
            </ul>
        </nav>
        <hr>
        <h1>Methodology</h1>

        <h2>1. Objective</h2>
        <p>
            The goal of this project is to develop a smart code assistant that can analyze Python code to either identify bugs and correct them or optimize the code for better time and space complexity. This assistant is designed with usability, automation, and intelligent feedback in mind.
        </p>

        <h2>2. Backend Intelligence: LLaMA 3.2:7B Model</h2>
        <p>
            At the heart of this application is the <strong>LLaMA 3.2:7B</strong> model, a powerful open-weight language model. The model runs locally and is accessed via a RESTful API provided by <code>Ollama</code>. 
            The prompt dynamically adapts based on user selection: 
        </p>
        <ul>
            <li><strong>Bug Finder Mode:</strong> “Find bugs in this code and give the correct version.”</li>
            <li><strong>Optimizer Mode:</strong> “Optimize the given code for best time and space complexity.”</li>
        </ul>

        <h2>3. Flask-based Backend</h2>
        <p>
            The backend is built using <strong>Flask</strong>, a lightweight and flexible Python web framework. The Flask server handles:
        </p>
        <ul>
            <li>Receiving code input and task type from the frontend</li>
            <li>Sending the prompt to the LLaMA model</li>
            <li>Returning the model’s response back to the frontend</li>
        </ul>

        <h2>4. Interactive Frontend</h2>
        <p>
            The user interface is designed for simplicity and clarity. It provides a text area for code input, along with options to choose between bug detection or optimization. The processed result is displayed clearly for easy comparison and understanding.
        </p>

        <h2>5. CI/CD with GitHub Actions</h2>
        <p>
            To maintain code quality and streamline development, <strong>GitHub Actions</strong> is used for Continuous Integration and Continuous Deployment (CI/CD). Every push or pull request triggers automated checks to ensure that:
        </p>
        <ul>
            <li>Code adheres to formatting and style guidelines</li>
            <li>Tests (if any) are passed before deployment</li>
            <li>Deployments are automated (in containerized setups or testing environments)</li>
        </ul>

        <h2>6. Dockerization and Deployment</h2>
        <p>
            To ensure platform independence and ease of deployment, the entire application — including the Flask server and its dependencies — is packaged into a <strong>Docker container</strong>. This image is then <strong>pushed to Docker Hub</strong> for public access. Users can pull the image and run the entire app with a single command, reducing setup friction and enhancing reproducibility.
        </p>
        <p>
            This approach enables seamless deployment across different environments, whether local, cloud-based, or within educational labs.
        </p>

        <h2>7. Local Model Deployment</h2>
        <p>
            The application avoids reliance on third-party APIs by running the LLaMA model locally. This ensures data privacy, minimal latency, and full control over inference behavior — a significant advantage in real-world deployments.
        </p>

        <h2>8. Summary</h2>
        <p>
            This methodology blends AI and software engineering best practices into a single cohesive application. By combining a powerful LLM with a user-friendly Flask interface, Docker-based deployment, and CI/CD automation, the project delivers a robust and production-ready tool that showcases real-world development practices and technical maturity.
        </p>
    </div>
</body>
</html>
